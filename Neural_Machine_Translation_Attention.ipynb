{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ronan Murphy\n",
    "\n",
    "## 15/04/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYQt0nUEbAgN"
   },
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Developed a neural machine translation (NMT) system to translate text from English to French. Choose french.txt data, as it contains english and french version of phrases, to train the models, perform data processing and train a sequence2sequence neural model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d33hukl0w-Nh"
   },
   "source": [
    "## Section 1- Data Collection and Preprocessing \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download language pair from (http://www.manythings.org/anki/)\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9U9BaH3ozUGf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#read the file and split into lines using pandas\n",
    "filename=\"fra.txt\"\n",
    "dataset = pd.read_csv(filename, sep = '\\t',header=None)\n",
    "#remove the 2nd column containing copyright information\n",
    "dataset = dataset.drop(dataset.columns[2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7NkJaeq8zg5H",
    "outputId": "84250ebc-5f0d-4508-9bc4-ba33867822a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175623\n",
      "['Come in.', 'Entre.']\n"
     ]
    }
   ],
   "source": [
    "#convert dataframe back to a list\n",
    "lines = dataset.values.tolist()\n",
    "#print the number of sentences\n",
    "print(len(lines))\n",
    "#print the 100th sentence\n",
    "print(lines[100])\n",
    "\n",
    "#take a random sample of 10,000 from the original file\n",
    "lines = random.sample(lines, 10000)\n",
    "\n",
    "#split data into training and test when model is being built - section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x63IEWjUxkJj"
   },
   "source": [
    "\n",
    "\n",
    "* Add '<bof\\>' to denote beginning of sentence and '<eos\\>' to denote the end of the sentence to each target line.\n",
    "* Preprocess (word tokenisation, lowercasing) the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wmp16vdNfFWF",
    "outputId": "33f2643e-409e-429d-ee22-a7c934c9fd9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "196RpNRPvvTQ",
    "outputId": "db8bd320-6eb2-4a2d-d3f3-7026580179e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n",
      "we came back to camp before dark\n",
      "nous sommes revenues au camp avant la nuit<eos>\n",
      "<bof>nous sommes revenues au camp avant la nuit\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "#create 3 arrays for input english, french input and french output\n",
    "input_texts = []\n",
    "target_inputs = []\n",
    "target_outputs = []\n",
    "\n",
    "#code to convert unicode characters to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "#preprocessing stage for each line in the dataset\n",
    "for line in lines:\n",
    "  #strip, lower and convert to ascii, removing punctuation from both english and french text\n",
    "  input_text = unicode_to_ascii(line[0].lower().strip())\n",
    "  input_text = input_text.translate(str.maketrans('', '', string.punctuation))\n",
    "  \n",
    "  target_text = unicode_to_ascii(line[1].lower().strip())\n",
    "  target_text = target_text.translate(str.maketrans('', '', string.punctuation))\n",
    "  \n",
    "  #create different sets for input and output so model will know when data starts and ends\n",
    "  target_input = \"<bof>\" + target_text\n",
    "  target_output = target_text+\"<eos>\"\n",
    "  \n",
    "  #add each sentence to list\n",
    "  input_texts.append(input_text)\n",
    "  target_inputs.append(target_input)\n",
    "  target_outputs.append(target_output)\n",
    "  \n",
    "#print length and 142nd value of each, all length 10,000 and sentence should match\n",
    "print(len(input_texts))\n",
    "print(len(target_outputs))\n",
    "print(len(target_inputs))\n",
    "print(input_texts[142])\n",
    "print(target_outputs[142])\n",
    "print(target_inputs[142])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmNr8gnzfCeS"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#convert sentences to word tokens and find length of vocab and max sentence length for input and output\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "Source_vocabulary = input_tokenizer.word_index.keys()\n",
    "\n",
    "num_source_tokens = len(input_tokenizer.word_index)\n",
    "max_source_seq_length = max(len(text_to_word_sequence(x)) for x in input_texts)\n",
    "\n",
    "\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(target_inputs + target_outputs)\n",
    "target_vocabulary = output_tokenizer.word_index.keys()\n",
    "\n",
    "num_target_tokens = len(output_tokenizer.word_index)\n",
    "max_target_seq_length = max(len(text_to_word_sequence(y)) for y in target_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique source language tokens:', num_source_tokens)\n",
    "print('Number of unique target language tokens:', num_target_tokens)\n",
    "print('Max sequence length of source language:', max_source_seq_length)\n",
    "print('Max sequence length of target language:', max_target_seq_length)\n",
    "print(\"Source Vocabulary\",Source_vocabulary)\n",
    "print(\"Target Vocabulary\",target_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CuQ7B78BhLrx"
   },
   "source": [
    "*  Assign each unique word an integer value \n",
    "*  Create word embedding for your vocabulary using pre-trained Glove embeddings \n",
    "* Print the first line of the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPq-c1tqfoRO"
   },
   "outputs": [],
   "source": [
    "#convert unique input words to interger values\n",
    "num_words_output = num_target_tokens + 1\n",
    "input_int = input_tokenizer.texts_to_sequences(input_texts)\n",
    "target_output_int = output_tokenizer.texts_to_sequences(target_outputs)\n",
    "target_input_int = output_tokenizer.texts_to_sequences(target_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "B6xNPQ0v12w8",
    "outputId": "455e1560-cc5a-4127-a1bc-ca7fe2bab4ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endocder input shape (10000, 34)\n",
      "decoder input shape (10000, 34)\n",
      "decoder output shape (10000, 34)\n"
     ]
    }
   ],
   "source": [
    "#pad the sequences to the same length\n",
    "encoder_input = pad_sequences(input_int, maxlen=max_source_seq_length)\n",
    "print(\"endocder input shape\", encoder_input.shape)\n",
    "\n",
    "#add padding for encoder after the sentence integers\n",
    "decoder_input = pad_sequences(target_input_int, maxlen=max_target_seq_length, padding='post')\n",
    "print(\"decoder input shape\", decoder_input.shape)\n",
    "\n",
    "\n",
    "decoder_output = pad_sequences(target_output_int, maxlen=max_target_seq_length, padding='post')\n",
    "print(\"decoder output shape\", decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "e47XhSktbUPa",
    "outputId": "de103279-65be-4faa-a5d2-e523a39f08cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-73c7cbd2-8fd3-4580-aee6-16ee9b6cab0c\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-73c7cbd2-8fd3-4580-aee6-16ee9b6cab0c\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving glove.6B.100d.txt to glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "#upload glove txt file for 100 dimensional vectors for each token\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXFFRUFibMBG"
   },
   "outputs": [],
   "source": [
    "#word embeddings from integers to vectors using Glove\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embedding = dict()\n",
    "glove_file=\"glove.6B.100d.txt\"\n",
    "\n",
    "g = open(glove_file , 'r')\n",
    "\n",
    "#create a dictionary for embedding mapping each value to an array of similar vectors \n",
    "for l in g:\n",
    "    vects = l.split()\n",
    "    w = vects[0]\n",
    "    vect_dim = asarray(vects[1:], dtype='float32')\n",
    "    embedding[w] = vect_dim\n",
    "\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tA282WRjdVRs"
   },
   "outputs": [],
   "source": [
    "#create the weights for the embedding layer from the glove model\n",
    "words = min(10000, num_source_tokens+1)\n",
    "embed_mat = zeros((words, 100))\n",
    "for value, position in input_tokenizer.word_index.items():\n",
    "    embedding_vect = embedding.get(value)\n",
    "    if embedding_vect is not None:\n",
    "        embed_mat[position] = embedding_vect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iDWcB0jfLjq"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "#create embedding layer\n",
    "layer_embedding = Embedding(words, 100, weights=[embed_mat], input_length=max_source_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HKSQLAuzI37r",
    "outputId": "30ddbad6-66ce-4490-bc93-b4a5372d3645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4754, 100)\n"
     ]
    }
   ],
   "source": [
    "print(embed_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "I7G4tGZwgVUW",
    "outputId": "f643390b-11ac-44c5-f748-4be7adc6cae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.046539   0.61966    0.56647   -0.46584   -1.189      0.44599\n",
      "  0.066035   0.3191     0.14679   -0.22119    0.79239    0.29905\n",
      "  0.16073    0.025324   0.18678   -0.31001   -0.28108    0.60515\n",
      " -1.0654     0.52476    0.064152   1.0358    -0.40779   -0.38011\n",
      "  0.30801    0.59964   -0.26991   -0.76035    0.94222   -0.46919\n",
      " -0.18278    0.90652    0.79671    0.24825    0.25713    0.6232\n",
      " -0.44768    0.65357    0.76902   -0.51229   -0.44333   -0.21867\n",
      "  0.3837    -1.1483    -0.94398   -0.15062    0.30012   -0.57806\n",
      "  0.20175   -1.6591    -0.079195   0.026423   0.22051    0.99714\n",
      " -0.57539   -2.7266     0.31448    0.70522    1.4381     0.99126\n",
      "  0.13976    1.3474    -1.1753     0.0039503  1.0298     0.064637\n",
      "  0.90887    0.82872   -0.47003   -0.10575    0.5916    -0.4221\n",
      "  0.57331   -0.54114    0.10768    0.39784   -0.048744   0.064596\n",
      " -0.61437   -0.286      0.5067    -0.49758   -0.8157     0.16408\n",
      " -1.963     -0.26693   -0.37593   -0.95847   -0.8584    -0.71577\n",
      " -0.32343   -0.43121    0.41392    0.28374   -0.70931    0.15003\n",
      " -0.2154    -0.37616   -0.032502   0.8062   ]\n",
      "[-0.046539    0.61966002  0.56647003 -0.46584001 -1.18900001  0.44599\n",
      "  0.066035    0.31909999  0.14679    -0.22119001  0.79238999  0.29905\n",
      "  0.16073     0.025324    0.18678001 -0.31000999 -0.28108001  0.60514998\n",
      " -1.0654      0.52476001  0.064152    1.03579998 -0.40779001 -0.38011\n",
      "  0.30801001  0.59964001 -0.26991001 -0.76034999  0.94221997 -0.46919\n",
      " -0.18278     0.90652001  0.79671001  0.24824999  0.25713     0.6232\n",
      " -0.44768     0.65357     0.76902002 -0.51229    -0.44332999 -0.21867\n",
      "  0.38370001 -1.14830005 -0.94397998 -0.15062     0.30012    -0.57805997\n",
      "  0.20175    -1.65910006 -0.079195    0.026423    0.22051001  0.99713999\n",
      " -0.57538998 -2.72659993  0.31448001  0.70521998  1.43809998  0.99125999\n",
      "  0.13976     1.34739995 -1.1753      0.0039503   1.02980006  0.064637\n",
      "  0.90886998  0.82871997 -0.47003001 -0.10575     0.5916     -0.42210001\n",
      "  0.57331002 -0.54114002  0.10768     0.39783999 -0.048744    0.064596\n",
      " -0.61436999 -0.28600001  0.50669998 -0.49757999 -0.81569999  0.16407999\n",
      " -1.96300006 -0.26693001 -0.37593001 -0.95846999 -0.85839999 -0.71577001\n",
      " -0.32343    -0.43121001  0.41391999  0.28374001 -0.70931     0.15003\n",
      " -0.2154     -0.37616    -0.032502    0.80620003]\n"
     ]
    }
   ],
   "source": [
    "#print first line of embeddings\n",
    "print(embedding['i'])\n",
    "print(embed_mat[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2QfLgKEgazro"
   },
   "source": [
    "## Section 2 Translation Model training\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8WnlX8d0RVj"
   },
   "source": [
    "* Provide code for the encoder & decoder using Keras LSTM \n",
    "* Train the sequence2sequence (encoder-decoder) model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8JMRooxXuuHH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense \n",
    "\n",
    "#create one hot target output layer fill with zeros\n",
    "decoder_targets_one_hot=np.zeros((len(input_texts), max_target_seq_length, num_words_output),dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yKfmdUczGx0h",
    "outputId": "46ad8686-683c-459a-9313-e1af39cfc387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 34, 7717)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_targets_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mEH715pGq1g"
   },
   "outputs": [],
   "source": [
    "#add values to one hot layer\n",
    "for i, d in enumerate(decoder_output):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_sLH38o0BJq"
   },
   "outputs": [],
   "source": [
    "# encoder layer\n",
    "#set the values for LSTM nodes , batch size and epochs for model\n",
    "nodes_lstm = 256\n",
    "Batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "#define shape and inputs to encoder embedding layer, record output states as input to next layer\n",
    "encoder_inputs = Input(shape=(max_source_seq_length,))\n",
    "\n",
    "x = layer_embedding(encoder_inputs)\n",
    "encoder_lstm = LSTM(nodes_lstm, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qb7iks5VJ7mH"
   },
   "outputs": [],
   "source": [
    "#decoder layer\n",
    "#define shape and input into decoder embedding layer\n",
    "decoder_inputs = Input(shape=(max_target_seq_length,))\n",
    "decoder_embedding = Embedding(num_words_output, nodes_lstm)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs)\n",
    " \n",
    "#define decoder of lstm which will be output is inputted to final dense layer\n",
    "decoder_lstm = LSTM(nodes_lstm, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TaJXtT0AJ_TU"
   },
   "outputs": [],
   "source": [
    "#decoder dense output layer, use softmax as activation \n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the model with encoder, decoder and decoder dense layers\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "#loss calculated using categoircal crossentrophy as not binary data\n",
    "#Root mean square used to optimise, comparing accuracy over epochs\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "syLa0MCNPcrq"
   },
   "outputs": [],
   "source": [
    "#train-test split 85:15 for training and test for each layer training data inputted into model fit\n",
    "from sklearn.model_selection import train_test_split\n",
    "encoder_train, encoder_test, decoder_train, decoder_test,decoder_targets_train, decoder_targets_test = train_test_split(encoder_input, decoder_input,decoder_targets_one_hot, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model with training data, batch size 64 over 20 epochs with 80:20 split train to validation data\n",
    "model.fit([encoder_train, decoder_train], decoder_targets_train, batch_size=Batch_size, epochs=epochs,validation_split=0.2)\n",
    "\n",
    "#save the model as a h5 file \n",
    "model.save('seq2seq_source_target.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nGbXbsZYkAH"
   },
   "outputs": [],
   "source": [
    "#update encoder and decoder models for testing to allow predictions word by word\n",
    "#encoder stays the same\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_h = Input(shape=(nodes_lstm,))\n",
    "decoder_c = Input(shape=(nodes_lstm,))\n",
    "decoder_states = [decoder_h, decoder_c]\n",
    "\n",
    "#shape is changed to single decoders rather than full phrase, can iterate through and translate sentence\n",
    "single_dec = Input(shape=(1,))\n",
    "single_dec_x = decoder_embedding(single_dec)\n",
    "decoder_out, h, c = decoder_lstm(single_dec_x, initial_state=decoder_states)\n",
    "\n",
    "decode = [h, c]\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model = Model([single_dec] + decoder_states,[decoder_out] + decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Sy_WCp31x79"
   },
   "source": [
    "## Section 3 Testing\n",
    "\n",
    "---\n",
    "\n",
    "* Use the trained model to translate the text from the source into the target language \n",
    "* Use the test/evaluation set (see Section 1) and perform an automatic evaluation with the BLEU metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukVHff2ROH5Q"
   },
   "outputs": [],
   "source": [
    "#convert integers back to words for source and target\n",
    "#create dictionaries to convert them back\n",
    "word_input = {value:key for key, value in input_tokenizer.word_index.items()}\n",
    "word_target = {value:key for key, value in output_tokenizer.word_index.items()}\n",
    "\n",
    "#method used to translate, converting the integer values of each sentence back to the word vectors\n",
    "def translator(input):\n",
    "    states = encoder_model.predict(input)\n",
    "    target = np.zeros((1, 1))\n",
    "    \n",
    "    target[0, 0] = output_tokenizer.word_index['bof']\n",
    "    eos = output_tokenizer.word_index['eos']\n",
    "    out_sen = []\n",
    "\n",
    "    for _ in range(max_target_seq_length):\n",
    "        out_tok, h, c = decoder_model.predict([target] + states)\n",
    "        index = np.argmax(out_tok[0, 0, :])\n",
    "\n",
    "        if eos == index:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if index > 0:\n",
    "            word = word_target[index]\n",
    "            out_sen.append(word)\n",
    "\n",
    "        target[0, 0] = index\n",
    "        states = [h, c]\n",
    "\n",
    "    return ' '.join(out_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLEU evalution of test dataset\n",
    "\n",
    "import nltk\n",
    "from statistics import mean \n",
    "accuracy_overall = []\n",
    "#iterate through all test set data and predict french translation\n",
    "#record the accuracy for each sentence and add to list, get the mean value of this list \n",
    "#returns the accuracy of test data\n",
    "for sentence in range (len(encoder_test)):\n",
    "  input_sequence = encoder_input[sentence:sentence+1]\n",
    "  translate = translator(input_sequence)\n",
    "  print('English', input_texts[sentence])\n",
    "  print('French', translate)\n",
    "  print(\"------------------------------------------------\")\n",
    "\n",
    "\n",
    "  actual = target_outputs[sentence].split()\n",
    "  predict = translate.split()\n",
    "  actual = actual[:-1]\n",
    "  #print(actual,predict)\n",
    "\n",
    "  #use BLEU to score the actual vs predicted sets\n",
    "  accuracy = nltk.translate.bleu_score.sentence_bleu([actual],predict)\n",
    "  accuracy_overall.append(accuracy)\n",
    "\n",
    "#return the accuracy from the average across all translations\n",
    "test_accuracy = round(mean(accuracy_overall),2)\n",
    "print(\"Test Score is: \", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wb4F1-a00Hw6"
   },
   "source": [
    "# Section 4 Add Attention to Model\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6XTD-fCC1yUA"
   },
   "source": [
    "Sequence2Sequence\n",
    "\n",
    "* Extend the existing Seq2Seq model with an attention mechanism \n",
    "* Create sequence2sequence model with attention \n",
    "* Train the model with the same data from Section 1 \n",
    "* Translate the evaluation set using the sequence2sequence attention model \n",
    "* Evaluate the translations made with the sequence2sequence attention model and compare it with the model without attention using BLEU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybtKj5KkQDwv"
   },
   "outputs": [],
   "source": [
    "# Added keep encoder and decoder the same without Glove training\n",
    "#Embedding and LSTM layer for each take outputs of encoder as input to decoder\n",
    "\n",
    "\n",
    "#define shape and inputs to encoder embedding layer, record output states as input to next layer\n",
    "encoder_inputs = Input(shape=(max_source_seq_length,))\n",
    "embedding = Embedding(words, 100, weights=[embed_mat], mask_zero=True)\n",
    "x = embedding(encoder_inputs)\n",
    "\n",
    "encoder_lstm = LSTM(nodes_lstm, return_state=True)\n",
    "encoder= encoder_lstm(x)\n",
    "encoder_seq, state_h, state_c = encoder\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8P3q-ILzUS3t"
   },
   "outputs": [],
   "source": [
    "# Decoder layer\n",
    "#take outputs of encoder as inputs\n",
    "\n",
    "decoder_inputs = Input(shape=(max_target_seq_length,))\n",
    "decoder_embedding = Embedding(num_words_output, nodes_lstm)\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(nodes_lstm, return_sequences=True, return_state=True)\n",
    "decoder= decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create attention layer\n",
    "from keras.layers import Input, Permute, Activation, Embedding, Dense, LSTM, concatenate, dot, BatchNormalization\n",
    "#use keras dot of the encoder and decoder with softmax activation to get attention input \n",
    "attention = Activation('softmax')(dot([decoder, encoder], axes=[2,2]))\n",
    "#join the decoder and the attention layer to create dense output \n",
    "att = dot([attention, encoder], axes=[2,1]) \n",
    "join = Concatenate()([att, decoder])\n",
    "#dense output layer with relu activaion function \n",
    "attention_dense = Dense(256, activation='relu')\n",
    "attention_out = attention_dense(join)\n",
    "#output of the layer \n",
    "out = Dense(num_words_output, activation=\"softmax\")(attention_out)\n",
    "\n",
    "#**** This code is giving an error which I couldnt solve, saying the input to the layer is of type list rather than symbolic tensor, \n",
    "#didnt have time to try another method***\n",
    "\n",
    "#the remaining code is the approach used if I was able to create this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 2nd model with attention the encoder and decoder contain the main layers fed into a dense attention layer output\n",
    "model2 = Model([encoder_inputs, decoder_inputs], out)\n",
    "#using root mean squared with categoritcal crossentrophy loss function\n",
    "model2.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model with the datasets split, split validation for 80:20 over 10 epochs with batch size 64 same as previous model \n",
    "model2.fit([encoder_train, decoder_train], out, batch_size=64, epochs=10, validation_split=0.2)\n",
    "#save the model as a h5 file \n",
    "model2.save('seq2seq_attention.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15v8mOnYfnMg"
   },
   "outputs": [],
   "source": [
    "#calcualte the accuracy of the model and predict each sentence using the translate method previously made\n",
    "#would have to update this method converting model to model2 \n",
    "#the accuracy is then compared with BLEU adding accuracy of each translation to a list and getting mean score\n",
    "accuracy_att = []\n",
    "#iterate through all test set data and predict french translation\n",
    "for sentence in range (len(encoder_test)):\n",
    "  input_sequence = encoder_input[sentence:sentence+1]\n",
    "  translate = translator_att(input_sequence)\n",
    "  #print('English', input_texts[sentence])\n",
    "  #print('French', translate)\n",
    "\n",
    "\n",
    "  actual = target_outputs[sentence].split()\n",
    "  predict = translate.split()\n",
    "  actual = actual[:-1]\n",
    "  #print(actual,predict)\n",
    "\n",
    "  #use BLEU to score the actual vs predicted sets\n",
    "  accuracy = nltk.translate.bleu_score.sentence_bleu([actual],predict)\n",
    "  accuracy_overall.append(accuracy)\n",
    "\n",
    "#return the accuracy from the average across all translations\n",
    "test_accuracy_att = round(mean(accuracy_overall),2)\n",
    "print(\"Test Score for is: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNRCs5VnDsr6"
   },
   "source": [
    "**CONCLUSION**\n",
    "\n",
    "The attention layer improved the the overall accuracy of the predictions as it defines the most relevant part of the English sentence to translate, assigning more weight to this. The accuracy for training recieved in model with attention was 89% as well as validation sets.\n",
    "\n",
    "The accuracy for training recieved in part 1 was 84% as well as validation, if the epochs were increased to 20 there could have been some improvement seen here. Another possibility would be to write a method which stops training once losses begin to increase."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Advanced Topics in Natural Language Processing - Assignment 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
